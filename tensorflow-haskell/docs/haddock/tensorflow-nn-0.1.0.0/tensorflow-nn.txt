-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Friendly layer around TensorFlow bindings.
--   
--   Please see README.md
@package tensorflow-nn
@version 0.1.0.0

module TensorFlow.NN

-- | Computes sigmoid cross entropy given <tt>logits</tt>.
--   
--   Measures the probability error in discrete classification tasks in
--   which each class is independent and not mutually exclusive. For
--   instance, one could perform multilabel classification where a picture
--   can contain both an elephant and a dog at the same time.
--   
--   For brevity, let `x = logits`, `z = targets`. The logistic loss is
--   
--   z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 <i>
--   (1 + exp(-x))) + (1 - z) * -log(exp(-x) </i> (1 + exp(-x))) = z *
--   log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z *
--   log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x +
--   log(1 + exp(-x)) = x - x * z + log(1 + exp(-x))
--   
--   For x &lt; 0, to avoid overflow in exp(-x), we reformulate the above
--   
--   x - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x))
--   = - x * z + log(1 + exp(x))
--   
--   Hence, to ensure stability and avoid overflow, the implementation uses
--   this equivalent formulation
--   
--   max(x, 0) - x * z + log(1 + exp(-abs(x)))
--   
--   <tt>logits</tt> and <tt>targets</tt> must have the same type and
--   shape.
sigmoidCrossEntropyWithLogits :: (OneOf '[Float, Double] a, TensorType a, Num a) => Tensor Value a -> Tensor Value a -> Build (Tensor Value a)
